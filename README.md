# Minimal LLM Chat UI + Proxy for Ollama

A lightweight, zero-install (static HTML) chat UI for Ollama, with a tiny Flask proxy backend for CORS support.  
Designed as a minimal, resource-friendly alternative to OpenWebUI.

## âœ¨ Features
- ğŸ“œ Chat with memory & session management
- ğŸ–¥ï¸ Works entirely in the browser (frontend is a single HTML file)
- âš¡ Ultra-lightweight Flask proxy (no database, no heavy server)
- ğŸ”„ Model loading, listing, and pulling directly from the UI
- ğŸ’¾ Save, import, and export chats (stored in browser localStorage)
- ğŸ“± Mobile-friendly responsive layout

## ğŸ“‚ Project Structure
minimal-llm-chat/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ ollama_proxy.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ minimal_llm_chat.html
â”œâ”€â”€ requirements.txt
